
My implementation of a rag agent

* Project Name: bragbrag

** Name Reasoning
In my friends group, when someone is talking about something awesome they did we often light heartedly respond with "brag brag"
So when I (Bray) wanted to make a RAG agent to learn about LLM models, brag brag was born
* Project Objectives
This project was used as a learning exersise. As I belive that the best way way to learn is to break things, so if you are trying to use this project, just know you have been warned.

My objective of this project is to create an RAG agent that utilizes a llama model.

I envision this project to have the following flow
- A user loads a document repository that they want an LLM model to utilize and reference when forming answers.
- A user asks a question from the command line prompt
- An agent retreives the relevent documents from the document repository
- A grader agent reviews the document and responds with a yes/no answer if the grader agent llm thinks the documents retrevied are relevent to the question. If the documents are not relevent it drops them from this response.
- A LLM agent generates a response to the original question using the relevent documen ts as reference material
- An grader agent then reviews the response, and responds with a yes/no answer if the grader LLM thinks the response is supported by the source material. If not it will trigger a re-generate.
- A grader agent will determine if the original question was answered, if not it will trigger a re-generate.
- If the response is both relevent to the source material, and answers the question a response will be played to the user.


[[https://github.com/bkm82/bragbrag/actions][https://github.com/bkm82/bragbrag/actions/workflows/tests.yml/badge.svg]]

* Setup/dependencies
** TODO I eventually this all into a package with these dependencies in the requeirements.txt
#+begin_src shell
  pip install -U langchain langchain_community tiktoken langchain-nomic "nomic[local]" langchain-ollama scikit-learn langgraph tavily-python bs4
#+end_src

ollama is used to get the LLM model https://ollama.com/download

 After it has been installed the model can be pulled using
 #+begin_src shell
   ollama pull ollama pull llama3.2:3b-instruct-fp16 
 #+end_src
* LLM code


#+BEGIN_SRC python :results output :exports both :session Python-Session
  from langchain_ollama import ChatOllama
  local_llm = 'llama3.2:3b' #todo, change this when the new one is installed
  llm = ChatOllama(model=local_llm, temperature=0)
  llm_json_mode = ChatOllama(model=local_llm, temperature=0, format='json')
#+end_src

#+RESULTS:


* Vector Store
This is how you can take the
This embeds the information from the documents 
#+begin_src python :results output :esports both :session Python-Session
  from langchain.text_splitter import RecursiveCharacterTextSplitter
  from langchain_community.document_loaders import WebBaseLoader
  from langchain_community.vectorstores import SKLearnVectorStore
  from langchain_nomic.embeddings import NomicEmbeddings

  urls = [
      "https://lilianweng.github.io/posts/2023-06-23-agent/",
      "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
      "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
  ]

  # Load documents
  docs = [WebBaseLoader(url).load() for url in urls]
  docs_list = [item for sublist in docs for item in sublist]

  # Split documents
  text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
      chunk_size=1000, chunk_overlap=200
  )
  doc_splits = text_splitter.split_documents(docs_list)

  # Add to vectorDB
  vectorstore = SKLearnVectorStore.from_documents(
      documents=doc_splits,
      embedding=NomicEmbeddings(model="nomic-embed-text-v1.5", inference_mode="local"),
  )

  # Create retriever
  retriever = vectorstore.as_retriever(k=3)
#+end_src
